{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81947a08-219e-4df9-98a6-2ff8d6102d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/mumbai_msw_data_2020_2024 (2).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/mumbai_msw_data_2020_2024 (2).csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns after reading CSV:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows, columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/mumbai_msw_data_2020_2024 (2).csv'"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Waste-to-Energy Prediction Pipeline with Ensembles\n",
    "# ==========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# Load Dataset\n",
    "# ---------------------------\n",
    "df = pd.read_csv(\"/content/mumbai_msw_data_2020_2024 (2).csv\")\n",
    "\n",
    "print(\"Columns after reading CSV:\", df.columns)\n",
    "print(f\"Loaded {df.shape[0]} rows, columns: {list(df.columns)}\")\n",
    "\n",
    "# Define target\n",
    "target_col = \"Estimated Energy Output (MWh)\"\n",
    "\n",
    "# Drop leakage-prone or non-predictive columns\n",
    "drop_cols = [\"Month-Year\", \"adj_lhv\", \"dry_fraction\"]\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# ---------------------------\n",
    "# Correlation check\n",
    "# ---------------------------\n",
    "corrs = X.corrwith(y).sort_values(ascending=False)\n",
    "print(\"\\nWarning: extremely high correlation detected between target and features (possible leakage):\")\n",
    "print(corrs[abs(corrs) > 0.95])\n",
    "\n",
    "# ---------------------------\n",
    "# Train / Validation / Test Split\n",
    "# ---------------------------\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.175, shuffle=False)\n",
    "\n",
    "print(f\"\\nData split -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions\n",
    "# ---------------------------\n",
    "def evaluate(model, X_val, y_val, model_name):\n",
    "    preds = model.predict(X_val)\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"R2\": r2_score(y_val, preds),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_val, preds)),\n",
    "        \"MAE\": mean_absolute_error(y_val, preds),\n",
    "        \"MAPE\": np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
    "    }, preds\n",
    "\n",
    "def evaluate_array(preds, y_true, name):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"R2\": r2_score(y_true, preds),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, preds)),\n",
    "        \"MAE\": mean_absolute_error(y_true, preds),\n",
    "        \"MAPE\": np.mean(np.abs((y_true - preds) / y_true)) * 100\n",
    "    }\n",
    "\n",
    "results, val_preds = [], {}\n",
    "\n",
    "# ---------------------------\n",
    "# Linear Regression\n",
    "# ---------------------------\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "res, preds = evaluate(lr, X_val, y_val, \"LinearRegression\")\n",
    "results.append(res); val_preds[\"LinearRegression\"] = preds\n",
    "\n",
    "# ---------------------------\n",
    "# Random Forest\n",
    "# ---------------------------\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42).fit(X_train, y_train)\n",
    "res, preds = evaluate(rf, X_val, y_val, \"RandomForest_Tuned\")\n",
    "results.append(res); val_preds[\"RandomForest\"] = preds\n",
    "\n",
    "# ---------------------------\n",
    "# XGBoost\n",
    "# ---------------------------\n",
    "xgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, subsample=0.8, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "res, preds = evaluate(xgb, X_val, y_val, \"XGBoost\")\n",
    "results.append(res); val_preds[\"XGBoost\"] = preds\n",
    "\n",
    "# ---------------------------\n",
    "# ANN\n",
    "# ---------------------------\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "ann.compile(optimizer=Adam(0.01), loss='mse')\n",
    "ann.fit(X_train, y_train, epochs=100, batch_size=8, verbose=0)\n",
    "preds = ann.predict(X_val).ravel()\n",
    "res = {\n",
    "    \"Model\": \"ANN\",\n",
    "    \"R2\": r2_score(y_val, preds),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_val, preds)),\n",
    "    \"MAE\": mean_absolute_error(y_val, preds),\n",
    "    \"MAPE\": np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
    "}\n",
    "results.append(res); val_preds[\"ANN\"] = preds\n",
    "\n",
    "# ---------------------------\n",
    "# SARIMAX (time-series)\n",
    "# ---------------------------\n",
    "# Fit on trainval to respect temporal ordering; forecast purely on len(y_val)\n",
    "sarimax = SARIMAX(y_trainval, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "sarimax_fit = sarimax.fit(disp=False)\n",
    "sarimax_preds = sarimax_fit.forecast(len(y_val)).values  # convert to np.array for consistency\n",
    "res = {\n",
    "    \"Model\": \"SARIMAX\",\n",
    "    \"R2\": r2_score(y_val, sarimax_preds),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_val, sarimax_preds)),\n",
    "    \"MAE\": mean_absolute_error(y_val, sarimax_preds),\n",
    "    \"MAPE\": np.mean(np.abs((y_val - sarimax_preds) / y_val)) * 100\n",
    "}\n",
    "results.append(res); val_preds[\"SARIMAX\"] = sarimax_preds\n",
    "\n",
    "# ---------------------------\n",
    "# Results Summary\n",
    "# ---------------------------\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Validation Performance Summary ===\\n\")\n",
    "print(res_df)\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Importance\n",
    "# ---------------------------\n",
    "def plot_feature_importance(model, X, title):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        sorted_idx = np.argsort(importances)\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.barh(range(len(sorted_idx)), importances[sorted_idx], align=\"center\")\n",
    "        plt.yticks(range(len(sorted_idx)), [X.columns[i] for i in sorted_idx])\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.show()\n",
    "\n",
    "plot_feature_importance(rf, X, \"Random Forest Feature Importance\")\n",
    "plot_feature_importance(xgb, X, \"XGBoost Feature Importance\")\n",
    "\n",
    "# ---------------------------\n",
    "# Actual vs Predicted (Validation)\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_val.values, label=\"Actual\", marker=\"o\")\n",
    "for model_name, preds in val_preds.items():\n",
    "    plt.plot(preds, label=model_name)\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted (Validation)\")\n",
    "plt.ylabel(\"Energy Output (MWh)\")\n",
    "plt.show()\n",
    "\n",
    "# ===========================================================\n",
    "# Ensemble Utilities\n",
    "# ===========================================================\n",
    "BASE_MODELS = [\"LinearRegression\", \"RandomForest\", \"XGBoost\", \"ANN\", \"SARIMAX\"]\n",
    "\n",
    "def stack_val_matrix(val_preds_dict, base_models=BASE_MODELS):\n",
    "    \"\"\"Construct validation prediction matrix of shape (n_samples, n_models).\"\"\"\n",
    "    return np.column_stack([val_preds_dict[m if m != \"RandomForest\" else \"RandomForest\"] for m in base_models])\n",
    "\n",
    "def simple_average(val_preds_dict, base_models=BASE_MODELS):\n",
    "    P = stack_val_matrix(val_preds_dict, base_models)\n",
    "    return np.mean(P, axis=1)\n",
    "\n",
    "def performance_weights(res_df, base_models=BASE_MODELS, metric=\"RMSE\"):\n",
    "    \"\"\"Compute weights inversely proportional to metric; robust to extreme values.\"\"\"\n",
    "    df = res_df.set_index(\"Model\")\n",
    "    # Map model names used in results to our BASE_MODELS keys\n",
    "    name_map = {\n",
    "        \"LinearRegression\":\"LinearRegression\",\n",
    "        \"RandomForest_Tuned\":\"RandomForest\",\n",
    "        \"XGBoost\":\"XGBoost\",\n",
    "        \"ANN\":\"ANN\",\n",
    "        \"SARIMAX\":\"SARIMAX\"\n",
    "    }\n",
    "    metrics = []\n",
    "    for m in base_models:\n",
    "        row_name = [k for k,v in name_map.items() if v == m][0]\n",
    "        metrics.append(df.loc[row_name, metric])\n",
    "    metrics = np.array(metrics, dtype=float)\n",
    "    # Stabilize inverse with epsilon\n",
    "    eps = 1e-8\n",
    "    inv = 1.0 / (metrics + eps)\n",
    "    w = inv / inv.sum()\n",
    "    return w  # shape (n_models,)\n",
    "\n",
    "def weighted_average(val_preds_dict, weights, base_models=BASE_MODELS):\n",
    "    P = stack_val_matrix(val_preds_dict, base_models)\n",
    "    return P @ weights\n",
    "\n",
    "def stacking_fit(P_val, y_val, alpha=1.0):\n",
    "    \"\"\"Fit a simple ridge meta-model on validation predictions.\"\"\"\n",
    "    meta = Ridge(alpha=alpha, random_state=42)\n",
    "    meta.fit(P_val, y_val)\n",
    "    return meta\n",
    "\n",
    "def ensemble_uncertainty(val_preds_dict, base_models=BASE_MODELS):\n",
    "    \"\"\"Estimate ensemble predictive variance across base models (epistemic proxy).\"\"\"\n",
    "    P = stack_val_matrix(val_preds_dict, base_models)\n",
    "    return P.var(axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Build Ensembles on Validation\n",
    "# ---------------------------\n",
    "P_val = stack_val_matrix(val_preds, BASE_MODELS)\n",
    "simple_avg_preds = simple_average(val_preds, BASE_MODELS)\n",
    "w_rmse = performance_weights(res_df, BASE_MODELS, metric=\"RMSE\")\n",
    "weighted_preds = weighted_average(val_preds, w_rmse, BASE_MODELS)\n",
    "meta_model = stacking_fit(P_val, y_val, alpha=1.0)\n",
    "stack_preds = meta_model.predict(P_val)\n",
    "val_var = ensemble_uncertainty(val_preds, BASE_MODELS)\n",
    "\n",
    "# Evaluate ensembles\n",
    "ens_results = []\n",
    "ens_results.append(evaluate_array(simple_avg_preds, y_val, \"Ensemble_SimpleAvg\"))\n",
    "ens_results.append(evaluate_array(weighted_preds, y_val, \"Ensemble_WeightedByRMSE\"))\n",
    "ens_results.append(evaluate_array(stack_preds, y_val, \"Ensemble_StackingRidge\"))\n",
    "\n",
    "ens_df = pd.DataFrame(ens_results)\n",
    "print(\"\\n=== Ensemble Validation Performance ===\\n\")\n",
    "print(ens_df)\n",
    "\n",
    "# Plot ensembles vs actual\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_val.values, label=\"Actual\", linewidth=2)\n",
    "plt.plot(simple_avg_preds, label=\"Ensemble_SimpleAvg\")\n",
    "plt.plot(weighted_preds, label=\"Ensemble_WeightedByRMSE\")\n",
    "plt.plot(stack_preds, label=\"Ensemble_StackingRidge\")\n",
    "plt.legend()\n",
    "plt.title(\"Ensemble Predictions vs Actual (Validation)\")\n",
    "plt.ylabel(\"Energy Output (MWh)\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Fit base models on full trainval for test evaluation\n",
    "# ---------------------------\n",
    "# Refit base models to trainval to avoid validation bleed when going to test\n",
    "lr_full = LinearRegression().fit(X_trainval, y_trainval)\n",
    "rf_full = RandomForestRegressor(n_estimators=200, random_state=42).fit(X_trainval, y_trainval)\n",
    "xgb_full = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, subsample=0.8, random_state=42)\n",
    "xgb_full.fit(X_trainval, y_trainval)\n",
    "\n",
    "ann_full = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_trainval.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "ann_full.compile(optimizer=Adam(0.01), loss='mse')\n",
    "ann_full.fit(X_trainval, y_trainval, epochs=100, batch_size=8, verbose=0)\n",
    "\n",
    "# SARIMAX already fit on trainval; need to forecast len(test)\n",
    "sarimax_test_preds = sarimax_fit.forecast(len(y_test)).values\n",
    "\n",
    "# Collect test predictions\n",
    "test_preds = {\n",
    "    \"LinearRegression\": lr_full.predict(X_test),\n",
    "    \"RandomForest\": rf_full.predict(X_test),\n",
    "    \"XGBoost\": xgb_full.predict(X_test),\n",
    "    \"ANN\": ann_full.predict(X_test).ravel(),\n",
    "    \"SARIMAX\": sarimax_test_preds\n",
    "}\n",
    "\n",
    "# Build test ensemble predictions using validation-derived weights/meta-model\n",
    "P_test = np.column_stack([test_preds[m] for m in BASE_MODELS])\n",
    "simple_avg_test = np.mean(P_test, axis=1)\n",
    "weighted_test = P_test @ w_rmse  # use same weights from validation RMSE\n",
    "stack_test = meta_model.predict(P_test)\n",
    "\n",
    "# Evaluate on test\n",
    "def eval_and_print(name, preds, y_true):\n",
    "    r = {\n",
    "        \"Model\": name,\n",
    "        \"R2\": r2_score(y_true, preds),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, preds)),\n",
    "        \"MAE\": mean_absolute_error(y_true, preds),\n",
    "        \"MAPE\": np.mean(np.abs((y_true - preds) / y_true)) * 100\n",
    "    }\n",
    "    print(f\"{name} -> R2: {r['R2']:.4f}, RMSE: {r['RMSE']:.4f}, MAE: {r['MAE']:.4f}, MAPE: {r['MAPE']:.2f}%\")\n",
    "    return r\n",
    "\n",
    "print(\"\\n=== Test Performance (Base Models) ===\")\n",
    "test_base_results = []\n",
    "for name, preds in test_preds.items():\n",
    "    test_base_results.append(eval_and_print(name, preds, y_test))\n",
    "\n",
    "print(\"\\n=== Test Performance (Ensembles) ===\")\n",
    "test_ens_results = []\n",
    "test_ens_results.append(eval_and_print(\"Ensemble_SimpleAvg\", simple_avg_test, y_test))\n",
    "test_ens_results.append(eval_and_print(\"Ensemble_WeightedByRMSE\", weighted_test, y_test))\n",
    "test_ens_results.append(eval_and_print(\"Ensemble_StackingRidge\", stack_test, y_test))\n",
    "\n",
    "# ---------------------------\n",
    "# Save models and ensembles\n",
    "# ---------------------------\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "\n",
    "joblib.dump(lr, \"./models/linear.pkl\")\n",
    "joblib.dump(rf, \"./models/rf.pkl\")\n",
    "joblib.dump(xgb, \"./models/xgb.pkl\")\n",
    "ann.save(\"./models/ann.h5\")\n",
    "sarimax_fit.save(\"./models/sarimax.pkl\")\n",
    "\n",
    "# Save refit models used for test-time inference\n",
    "joblib.dump(lr_full, \"./models/linear_full.pkl\")\n",
    "joblib.dump(rf_full, \"./models/rf_full.pkl\")\n",
    "joblib.dump(xgb_full, \"./models/xgb_full.pkl\")\n",
    "ann_full.save(\"./models/ann_full.h5\")\n",
    "\n",
    "# Save ensemble artifacts\n",
    "np.save(\"./models/ensemble_weights_rmse.npy\", w_rmse)\n",
    "joblib.dump(meta_model, \"./models/stacking_ridge.pkl\")\n",
    "\n",
    "print(\"\\nSaved trained models and ensemble artifacts in ./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47e9e0-c9cf-45e8-8a8a-8132814778f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
